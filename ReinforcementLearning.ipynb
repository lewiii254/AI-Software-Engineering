{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e939f-2c07-4001-82a5-dee3ad5e1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    " \n",
    "\n",
    "# Create the CartPole environment\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    " \n",
    "\n",
    "# Define the Q-learning agent\n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.gamma = 0.95  # discount rate\n",
    "\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model = self._build_model()\n",
    "\n",
    " \n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        return model\n",
    "\n",
    " \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    " \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        act_values = self.model.predict(state)\n",
    "\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    " \n",
    "\n",
    "    def replay(self, batch_size):\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "\n",
    "            target = reward\n",
    "\n",
    "            if not done:\n",
    "\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    " \n",
    "\n",
    "# Initialize the agent\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    " \n",
    "\n",
    "# Train the agent\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "episodes = 1000\n",
    "\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    for time in range(500):\n",
    "\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        reward = reward if not done else -10\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "\n",
    "            print(f\"Episode: {e}/{episodes}, Score: {time}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "            break\n",
    "\n",
    "    if len(agent.memory) > batch_size:\n",
    "\n",
    "        agent. Replay(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
